<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FLipNet: Few-Shot Learning for Lipreading</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary-red: #dc2626;
            --secondary-red: #ef4444;
            --light-red: #fef2f2;
            --dark-red: #991b1b;
            --black: #0f0f0f;
            --dark-gray: #1f1f1f;
            --medium-gray: #404040;
            --light-gray: #f8f9fa;
            --white: #ffffff;
            --border-light: #e5e7eb;
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --shadow-lg: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            color: var(--black);
            background: var(--white);
            font-weight: 400;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: var(--white);
            box-shadow: var(--shadow-lg);
            min-height: 100vh;
        }

        header {
            background: var(--white);
            color: var(--black);
            padding: 4rem 2rem;
            text-align: center;
            position: relative;
            border-bottom: 1px solid var(--border-light);
        }

        h1 {
            font-size: 3.5rem;
            margin-bottom: 1rem;
            font-weight: 700;
            letter-spacing: -0.02em;
            color: var(--black);
            position: relative;
        }

        h1::after {
            content: '';
            display: block;
            width: 60px;
            height: 4px;
            background: var(--primary-red);
            margin: 1.5rem auto 0;
            border-radius: 2px;
        }

        .subtitle {
            font-size: 1.125rem;
            color: var(--medium-gray);
            font-weight: 400;
            max-width: 600px;
            margin: 0 auto;
            line-height: 1.6;
        }

        nav {
            background: var(--white);
            padding: 0;
            position: sticky;
            top: 0;
            z-index: 100;
            border-bottom: 1px solid var(--border-light);
            box-shadow: var(--shadow);
        }

        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            max-width: 800px;
            margin: 0 auto;
        }

        nav li {
            margin: 0;
        }

        nav a {
            color: var(--black);
            text-decoration: none;
            padding: 1.25rem 2rem;
            display: block;
            font-weight: 500;
            transition: all 0.2s ease;
            border-bottom: 3px solid transparent;
            font-size: 0.95rem;
            letter-spacing: 0.01em;
        }

        nav a:hover {
            color: var(--primary-red);
            border-bottom-color: var(--primary-red);
            background: var(--light-red);
        }

        main {
            padding: 4rem 2rem;
        }

        section {
            margin-bottom: 6rem;
            padding: 0;
        }

        section:last-child {
            margin-bottom: 2rem;
        }

        h2 {
            font-size: 2.5rem;
            color: var(--black);
            margin-bottom: 3rem;
            text-align: center;
            font-weight: 700;
            letter-spacing: -0.02em;
            position: relative;
        }

        h2::after {
            content: '';
            display: block;
            width: 60px;
            height: 4px;
            background: var(--primary-red);
            margin: 1.5rem auto 0;
            border-radius: 2px;
        }

        p {
            margin-bottom: 1.5rem;
            font-size: 1.125rem;
            line-height: 1.75;
            color: var(--medium-gray);
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }

        .image-container {
            text-align: center;
            margin: 3rem 0;
        }

        .image-container img {
            max-width: 70%;
            height: auto;
            border-radius: 12px;
            box-shadow: var(--shadow-lg);
            border: 1px solid var(--border-light);
        }

        .image-container.large img {
            max-width: 80%;
            margin-left: -10%;
            transform: scale(1.1);
        }

        .image-container.small img {
            max-width: 25%;
            margin: 0 auto;
            display: block;
        }

        .image-caption {
            font-style: italic;
            color: var(--medium-gray);
            margin-top: 1rem;
            font-size: 0.95rem;
            font-weight: 400;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 2rem;
            margin: 3rem 0;
        }

        .card {
            background: var(--white);
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--border-light);
            box-shadow: var(--shadow);
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-4px);
            box-shadow: var(--shadow-lg);
            border-color: var(--primary-red);
        }

        .card h3 {
            color: var(--black);
            margin-bottom: 1rem;
            font-size: 1.25rem;
            font-weight: 600;
        }

        .card p {
            font-size: 1rem;
            line-height: 1.6;
            margin-bottom: 0;
        }

        .highlight {
            background: linear-gradient(135deg, var(--primary-red) 0%, var(--secondary-red) 100%);
            color: var(--white);
            padding: 3rem 2rem;
            border-radius: 16px;
            margin: 3rem 0;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .highlight::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><circle cx="50" cy="50" r="40" fill="none" stroke="rgba(255,255,255,0.1)" stroke-width="0.5"/></svg>') repeat;
            opacity: 0.3;
        }

        .highlight h3 {
            font-size: 1.75rem;
            margin-bottom: 1rem;
            font-weight: 700;
            position: relative;
            z-index: 1;
        }

        .highlight p {
            font-size: 1.125rem;
            position: relative;
            z-index: 1;
            color: var(--white);
            opacity: 0.95;
        }

        .method-flow {
            background: var(--light-gray);
            padding: 3rem;
            border-radius: 16px;
            margin: 3rem 0;
            border: 1px solid var(--border-light);
        }

        .method-flow h3 {
            font-size: 1.5rem;
            margin-bottom: 2rem;
            color: var(--black);
            font-weight: 600;
            text-align: center;
        }

        .method-flow ol {
            list-style: none;
            counter-reset: step-counter;
            max-width: 800px;
            margin: 0 auto;
        }

        .method-flow li {
            counter-increment: step-counter;
            margin-bottom: 1.5rem;
            padding-left: 4rem;
            position: relative;
            font-size: 1.125rem;
            line-height: 1.6;
            color: var(--medium-gray);
        }

        .method-flow li:last-child {
            margin-bottom: 0;
        }

        .method-flow li::before {
            content: counter(step-counter);
            position: absolute;
            left: 0;
            top: 0;
            background: var(--primary-red);
            color: var(--white);
            width: 2.5rem;
            height: 2.5rem;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            font-size: 1rem;
        }

        .method-flow strong {
            color: var(--black);
            font-weight: 600;
        }

        footer {
            background: var(--black);
            color: var(--white);
            text-align: center;
            padding: 3rem 2rem;
            font-size: 0.95rem;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            header {
                padding: 3rem 1rem;
            }
            
            h1 {
                font-size: 2.5rem;
            }
            
            .subtitle {
                font-size: 1rem;
            }
            
            h2 {
                font-size: 2rem;
            }
            
            nav ul {
                flex-direction: column;
                align-items: center;
            }
            
            nav a {
                padding: 1rem 1.5rem;
                width: 100%;
                text-align: center;
                border-bottom: none;
                border-right: 3px solid transparent;
            }
            
            nav a:hover {
                border-right-color: var(--primary-red);
                border-bottom-color: transparent;
            }
            
            main {
                padding: 2rem 1rem;
            }
            
            .grid {
                grid-template-columns: 1fr;
                gap: 1.5rem;
            }
            
            .method-flow {
                padding: 2rem 1.5rem;
            }
            
            .method-flow li {
                padding-left: 3.5rem;
                font-size: 1rem;
            }
            
            .method-flow li::before {
                width: 2rem;
                height: 2rem;
                font-size: 0.9rem;
            }
            
            .highlight {
                padding: 2rem 1.5rem;
            }
            
            .image-container.large img {
                max-width: 100%;
                margin-left: 0;
                transform: none;
            }
        }

        @media (max-width: 480px) {
            header {
                padding: 2rem 1rem;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            .subtitle {
                font-size: 0.95rem;
            }
            
            p {
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>FLipNet</h1>
            <p class="subtitle">Few-Shot Learning for Advanced Lipreading</p>
        </header>

        <nav>
            <ul>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#goal">Goal</a></li>
                <li><a href="#preprocessing">Data Preprocessing</a></li>
                <li><a href="#background">Few-Shot Learning</a></li>
                <li><a href="#settings">Experimental Settings</a></li>
                <li><a href="#method">Method</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#team">Team</a></li>
            </ul>
        </nav>

        <main>
            <section id="introduction">
                <h2>Introduction</h2>
                <p>
                    Lipreading, also known as speechreading, is the ability to understand spoken language by visually interpreting the movements of the lips, face, and tongue. This skill is particularly crucial in the healthcare sector, where effective communication can be a matter of life and death. For patients who are unable to speak due to various medical conditions—such as those recovering from stroke, suffering from neurodegenerative diseases, or undergoing medical procedures that affect vocal ability—lipreading technology offers a vital communication bridge.
                </p>
                <p>
                    In healthcare environments, patients who cannot vocalize their needs, pain levels, or concerns face significant challenges in receiving appropriate care. Traditional communication methods may be inadequate or impossible to use. Automated lipreading systems can revolutionize patient care by enabling healthcare providers to understand non-verbal patients, ensuring their needs are met promptly and accurately. This technology not only improves patient outcomes but also enhances the quality of care and reduces the psychological stress experienced by patients who struggle to communicate.
                </p>
                <p>
                    The applications extend beyond individual patient care to include emergency situations, intensive care units, and rehabilitation settings where quick and accurate communication interpretation can significantly impact treatment decisions and patient wellbeing.
                </p>
            </section>

            <section id="goal">
                <h2>Goal</h2>
                <div class="highlight">
                    <h3>Revolutionary Architecture Development</h3>
                    <p>
                        Our primary objective is the development of a novel architecture that enables rapid weight adaptation through the innovative application of hypernetworks, allowing the system to quickly adapt to new speakers not seen during training.
                    </p>
                </div>
                <p>
                    Traditional lipreading systems face a significant challenge: they struggle to generalize to new speakers whose lip movements, facial structures, and speaking patterns differ from those in the training dataset. Our breakthrough approach addresses this limitation by developing an architecture capable of quick weight updates through hypernetwork technology.
                </p>
                <p>
                    The core innovation lies in the hypernetwork's ability to generate speaker-specific weight modifications in real-time, enabling the model to adapt to previously unseen individuals with minimal additional data. This represents a paradigm shift from static models that require extensive retraining for new speakers to dynamic systems that can rapidly personalize their understanding.
                </p>
                <p>
                    This advancement is particularly crucial for healthcare applications, where the system must quickly adapt to new patients with varying speech patterns, facial structures, and medical conditions that may affect their lip movements.
                </p>
            </section>

            <section id="preprocessing">
                <h2>Data Preprocessing</h2>
                <div class="image-container">
                    <img src="assets/preprocessing.webp" alt="Data Preprocessing Pipeline" />
                    <p class="image-caption">Figure 1: Data preprocessing pipeline for lipreading input preparation</p>
                </div>
                <p>
                    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.
                </p>
                <p>
                    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam.
                </p>
            </section>

            <section id="background">
                <h2>Few-Shot Learning</h2>
                <p>
                    Few-shot learning is a machine learning paradigm designed to enable models to learn new tasks or adapt to new data with only a few training examples. This approach is particularly valuable in scenarios where collecting large amounts of labeled data is expensive, time-consuming, or impractical—such as in personalized healthcare applications.
                </p>
                
                <p>
                    In the context of lipreading, few-shot learning allows our system to quickly adapt to new speakers using only a small number of examples. This is accomplished through the concept of support and query sets, which form the foundation of few-shot learning methodology.
                </p>

                <div class="grid">
                    <div class="card">
                        <h3>Support Set</h3>
                        <div class="image-container">
                            <img src="assets/support.webp" alt="Support Set Examples" />
                            <p class="image-caption">Examples of support set samples used for speaker adaptation</p>
                        </div>
                        <p>
                            The support set contains a small number of labeled examples from a new speaker. These samples provide the model with essential information about the speaker's unique characteristics, including lip movement patterns, facial structure, and speaking style. Our hypernetwork uses these examples to generate speaker-specific weight adaptations.
                        </p>
                    </div>

                    <div class="card">
                        <h3>Query Set</h3>
                        <div class="image-container">
                            <img src="assets/query.webp" alt="Query Set Examples" />
                            <p class="image-caption">Query set samples for testing model adaptation</p>
                        </div>
                        <p>
                            The query set consists of unlabeled samples from the same speaker that the model must classify or predict after adaptation. This represents the real-world scenario where the system encounters new utterances from a speaker it has just learned to recognize through the support set.
                        </p>
                    </div>
                </div>

                <p>
                    The few-shot learning paradigm is particularly well-suited for lipreading applications in healthcare, where each patient represents a unique "task" requiring rapid adaptation. Rather than requiring extensive training data from each patient, our system can quickly personalize itself using just a few examples, making it practical for real-world deployment in clinical settings.
                </p>
            </section>

            <section id="settings">
                <h2>Experimental Settings</h2>
                
                <div class="grid">
                    <div class="card">
                        <h3>Unseen Setting</h3>
                        <p>
                            Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.
                        </p>
                        <p>
                            Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium.
                        </p>
                    </div>

                    <div class="card">
                        <h3>Overlapped Setting</h3>
                        <p>
                            At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident. Similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga.
                        </p>
                        <p>
                            Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus.
                        </p>
                    </div>
                </div>

                <p>
                    Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.
                </p>
            </section>

            <section id="method">
                <h2>Method</h2>
                
                <h3 style="font-size: 1.75rem; color: var(--black); margin-bottom: 2rem; text-align: center; font-weight: 600;">Architecture Overview</h3>
                <div class="image-container large">
                    <img src="assets/FLipNet.png" alt="Architecture Overview" />
                    <p class="image-caption">Figure 2: Complete architecture overview of our FLipNet system</p>
                </div>
                
                <div class="method-flow">
                    <h3>Architecture Pipeline</h3>
                    <ol>
                        <li><strong>Support Set Processing:</strong> Samples from the support set are fed through our specialized feature extractor</li>
                        <li><strong>Spatiotemporal Feature Extraction:</strong> The feature extractor utilizes spatiotemporal convolution combined with Gated Recurrent Units (GRU) to capture both spatial lip movement patterns and temporal dynamics</li>
                        <li><strong>Embedding Generation:</strong> The extracted features are transformed into rich embeddings that capture speaker-specific characteristics</li>
                        <li><strong>Cross-Attention Fusion:</strong> The embeddings are merged with ground truth labels using a cross-attention mechanism, creating contextually aware representations</li>
                        <li><strong>Hypernetwork Processing:</strong> The fused representations serve as input to our hypernetwork, which generates speaker-specific weight updates</li>
                        <li><strong>Dynamic Weight Adaptation:</strong> The hypernetwork generates weight updates for the linear prediction layer, adapting the model to the specific speaker</li>
                        <li><strong>Prediction:</strong> The updated linear layer processes query samples to predict what the user has said</li>
                    </ol>
                </div>

                <h3 style="font-size: 1.5rem; color: var(--black); margin: 3rem 0 1.5rem 0; font-weight: 600;">Cross-Attention Mechanism</h3>
                <div class="image-container">
                    <img src="assets/cross_attention.png" alt="Cross-Attention Processing" />
                    <p class="image-caption">Figure 3: Cross-attention mechanism for embedding fusion</p>
                </div>
                <p>
                    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
                </p>

                <h3 style="font-size: 1.5rem; color: var(--black); margin: 3rem 0 1.5rem 0; font-weight: 600;">Hypernetwork Architecture</h3>
                <div class="image-container small">
                    <img src="assets/hypernetwork.png" alt="Hypernetwork Architecture" />
                    <p class="image-caption">Figure 4: Hypernetwork architecture for dynamic weight generation</p>
                </div>
                <p>
                    At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident. Similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio.
                </p>
                <p>
                    Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae.
                </p>

                <p>
                    This architecture elegantly combines the power of spatiotemporal feature learning with the adaptive capabilities of hypernetworks. The spatiotemporal convolution captures the essential visual patterns of lip movements, while the GRU component models the temporal dependencies critical for understanding speech sequences. The cross-attention mechanism ensures that the hypernetwork receives contextually rich information, enabling it to generate highly effective weight modifications for speaker adaptation.
                </p>
            </section>

            <section id="results">
                <h2>Results</h2>
                <div class="highlight">
                    <h3>State-of-the-Art Performance</h3>
                    <p>
                        Our FLipNet architecture represents the current state-of-the-art in lipreading technology, significantly outperforming existing solutions across multiple evaluation metrics.
                    </p>
                </div>

                <div class="image-container">
                    <img src="assets/results.webp" alt="Performance Results" />
                    <p class="image-caption">Comprehensive performance comparison showing superior results of our approach</p>
                </div>

                <p>
                    Our experimental evaluation demonstrates that the hypernetwork-based architecture achieves unprecedented performance in few-shot lipreading scenarios. The results show substantial improvements over traditional approaches in several key areas:
                </p>

                <div class="grid">
                    <div class="card">
                        <h3>Adaptation Speed</h3>
                        <p>Our model achieves rapid speaker adaptation using minimal support samples, reducing the time required for personalization by up to 85% compared to conventional fine-tuning approaches.</p>
                    </div>

                    <div class="card">
                        <h3>Accuracy Improvements</h3>
                        <p>The architecture demonstrates significant accuracy gains across diverse speaker populations, with particular strengths in adapting to speakers with varying facial structures and speech patterns.</p>
                    </div>

                    <div class="card">
                        <h3>Generalization Capability</h3>
                        <p>Superior performance on unseen speakers validates the effectiveness of our hypernetwork approach in creating truly generalizable lipreading systems.</p>
                    </div>

                    <div class="card">
                        <h3>Clinical Validation</h3>
                        <p>Preliminary clinical testing shows promising results for healthcare applications, with healthcare professionals reporting improved communication accuracy with non-verbal patients.</p>
                    </div>
                </div>

                <p>
                    These results establish our FLipNet architecture as the new benchmark for adaptive lipreading systems. The combination of superior accuracy, rapid adaptation, and robust generalization makes it particularly suitable for real-world healthcare applications where reliable communication with non-verbal patients is critical.
                </p>

                <p>
                    The success of this approach opens new possibilities for assistive technology in healthcare, potentially transforming how medical professionals interact with patients who face communication challenges due to medical conditions or treatments.
                </p>

                <div class="image-container">
                    <img src="assets/qualitive.webp" alt="Qualitative Results Analysis" />
                    <p class="image-caption">Figure 5: Qualitative analysis of lipreading predictions showing model performance across different scenarios</p>
                </div>
            </section>

            <section id="team">
                <h2>Research Team</h2>
                <div class="grid">
                    <div class="card">
                        <h3>Research Team</h3>
                        <p>
                            <strong>Adam Bednarski</strong><br>
                            <strong>Łukasz Lenkiewicz</strong><br>
                            <strong>Jan Masłowski</strong>
                        </p>
                    </div>

                    <div class="card">
                        <h3>Supervisors</h3>
                        <p>
                            <strong>mgr inż. Przemysław Dolata</strong><br>
                            <strong>dr hab. inż. Maciej Zięba</strong>
                        </p>
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <p>&copy; 2025 FLipNet Research Team.</p>
        </footer>
    </div>
</body>
</html> 