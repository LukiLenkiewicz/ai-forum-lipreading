<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <link rel="stylesheet" href="styles.css">
    <title>FLipNet: Few-Shot Learning for Lipreading</title>
</head>
<body>
    <div class="container">
        <header>
            <h1>FLipNet</h1>
            <p class="subtitle">Wykorzystanie podejścia typu few-shot learning do zadania lipreadingu</p>
        </header>
    
        <nav id="navbar">
            <div class="nav-container">
                <button class="hamburger" id="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
                <ul id="nav-menu">
                    <li><a href="#introduction">Wprowadzenie</a></li>
                    <li><a href="#goal">Cel</a></li>
                    <li><a href="#preprocessing">Przetwarzanie danych</a></li>
                    <li><a href="#background">Uczenie przy małej liczbie przykładów</a></li>
                    <li><a href="#settings">Ustawienia eksperymentów</a></li>
                    <li><a href="#method">Metoda</a></li>
                    <li><a href="#metrics">Metryka</a></li>
                    <li><a href="#results">Wyniki</a></li>
                    <li><a href="#team">Zespół</a></li>
                </ul>
            </div>
        </nav>    

        <main>
            <section id="introduction">
                <h2>Wprowadzenie</h2>
                <p><strong>Odczytywanie mowy z ruchu warg</strong> (ang. <em>lipreading</em>) to proces rozpoznawania wypowiedzi na podstawie ruchów ust mówcy. Choć dla większości z nas jest to umiejętność wspomagająca słuch (efekt McGurka), to staje się ona kluczowa, w sytuacjach, gdy dźwięk jest niedostępny lub zakłócony. Technologia lipreadingu ma ogromny potencjał – może wspierać komunikację w hałaśliwym otoczeniu, a także umożliwiać porozumiewanie się osobom, które nie są w stanie komunikować się werbalnie, np. pacjenci po operacjach lub z zaburzeniami neurologicznymi.</p>
              
                <p>W naszej pracy prezentujemy nowatorski model do odczytywania mowy z ruchu warg. Opracowana metoda wykorzystuje <em>hipersieć</em> (ang. <em>hypernetwork</em>) do dynamicznej aktualizacji wag głównej architektury, co umożliwia <strong>skuteczną adaptację do nowych użytkowników</strong>. Dzięki temu model potrafi efektywnie przystosować się do indywidualnych cech mimiki konkretnej osoby, co przekłada się na wyższą dokładność predykcji.</p>
              
                <p>Nasze rozwiązanie zostało zweryfikowane na zbiorze danych <strong>GRID</strong>, będącym standardem w badaniach nad lipreadingiem. Osiągnięte wyniki <strong>przewyższają najlepsze dotychczasowe rozwiązania</strong>. Otwiera to nowe możliwości zastosowań – od cichych interfejsów głosowych, przez rozpoznawanie mowy w hałaśliwym otoczeniu, aż po technologie wspomagające osoby z zaburzeniami mowy.</p>
              </section>              

            <section id="preprocessing">
                <h2>Przygotowanie danych</h2>
                <div class="image-container">
                  <img src="assets/preprocessing.webp" alt="preprocessing" />
                  <p class="image-caption">Przykład obszaru wyciętego z danej klatki nagrania w procesie przygotowania danych.</p>
                </div>
                <p>W celu zwiększenia skuteczności modelu i usunięcia zbędnego szumu, zastosowaliśmy preprocessing nagrań wideo z wykorzystaniem bibliotek <strong>OpenCV</strong> oraz <strong>MediaPipe</strong>. Dla każdego nagrania automatycznie wykrywaliśmy twarz, a następnie wycinaliśmy regiony odpowiadające ustom mówcy. Dodatkowo, zmniejszenie rozmiaru wejściowych danych przekłada się na <strong>niższe zapotrzebowanie na zasoby obliczeniowe</strong> i szybsze działanie całego systemu.</p>
            </section>

            <section id="background">
                <h2>Few-Shot Learning</h2>
                <p>
                  Few-shot learning to paradygmat uczenia maszynowego, który umożliwia modelom dostosowywanie się do nowych zadań przy użyciu jedynie kilku przykładów uczących. Podejście to jest szczególnie przydatne w sytuacjach, gdy zebranie dużej ilości oznaczonych danych jest kosztowne, czasochłonne lub trudne do zrealizowania.
                </p>
              
                <p>
                  W kontekście odczytywania mowy z ruchu warg, few-shot learning pozwala naszemu systemowi szybko przystosować się do nowego mówcy, korzystając z ograniczonej liczby przykładów. Realizujemy to poprzez wykorzystanie koncepcji zbioru wsparcia (ang. <em>support set</em>) i zbioru zapytań (ang. <em>query set</em>), które stanowią fundament tej metodyki.
                </p>
              
                <div class="grid">
                  <div class="card">
                    <h3>Zbiór wsparcia</h3>
                    <div class="image-container">
                      <img src="assets/support.webp" alt="Przykłady ze zbioru wsparcia" />
                      <p class="image-caption">Przykłady używane w celu dostosowania modelu do nowego mówcy.</p>
                    </div>
                    <p>
                      Zbiór wsparcia zawiera niewielką liczbę oznaczonych przykładów od nowego mówcy. Próbki te dostarczają modelowi kluczowych informacji o indywidualnych cechach użytkownika, takich jak wzorce ruchu warg, struktura twarzy czy styl artykulacji. Nasza hipersieć wykorzystuje te dane do wygenerowania aktualizacji wag modelu.
                    </p>
                  </div>
              
                  <div class="card">
                    <h3>Zbiór zapytań</h3>
                    <div class="image-container">
                      <img src="assets/query.webp" alt="Przykłady ze zbioru zapytań" />
                      <p class="image-caption">Przykłady służące do oceny skuteczności adaptacji modelu.</p>
                    </div>
                    <p>
                      Zbiór zapytań składa się z nieoznaczonych próbek tego samego mówcy, które model musi rozpoznać po dokonaniu adaptacji. Taki schemat odzwierciedla rzeczywisty scenariusz, w którym system napotyka nowe wypowiedzi osoby, do której dopiero co został dostosowany.
                    </p>
                  </div>
                </div>
              </section>

              <section id="method">
                <h2>Metoda</h2>
                <div class="image-container large">
                  <img src="assets/FLipNet.png" alt="Architektury" />
                  <p class="image-caption">Schemat kompletnej architektury systemu FLipNet.</p>
                </div>
              
                <div class="method-flow">
                  <h3>Przebieg działania architektury</h3>
                  <ol>
                    <li><strong>Ekstrakcja cech czasowo-przestrzennych:</strong> Ekstraktor wykorzystuje konwolucje czasowo-przestrzenne (ang. <em>spatiotemporal convolutions</em>) w połączeniu z rekurencyjnymi jednostkami GRU, aby uchwycić zarówno wzorce ruchu warg, jak i dynamikę czasową.</li>
                    <li><strong>Fuzja z wykorzystaniem mechanizmu uwagi krzyżowej (ang. <em>cross-attention</em>):</strong> Osadzenia są łączone z etykietami przy użyciu mechanizmu uwagi krzyżowej, co pozwala uzyskać reprezentację wektorową całego zbioru wsparcia.</li>
                    <li><strong>Przetwarzanie przez hipersieć:</strong> Uzyskana reprezentacja wektorowa stanowi wejście do hipersieci, która generuje dostosowane do mówcy aktualizacje wag modelu bazowego.</li>
                    <li><strong>Predykcja:</strong> Zaktualizowana warstwa predykcyjna analizuje próbki ze zbioru zapytań w celu rozpoznania wypowiedzi.</li>
                  </ol>
                </div>
              
                <h3 style="font-size: 1.5rem; color: var(--black); margin: 3rem 0 1.5rem 0; font-weight: 600;">Mechanizm uwagi krzyżowej</h3>
                <div class="image-container">
                  <img src="assets/cross_attention.png" alt="Przetwarzanie za pomocą cross-attention" />
                  <p class="image-caption">Mechanizm uwagi krzyżowej stosowany do fuzji osadzeń.</p>
                </div>
                <p>
                  W opracowanej metodzie zastosowano mechanizm uwagi krzyżowej pomiędzy dwoma modalnościami: wizualną oraz tekstową. Umożliwia to wykorzystanie relacji między ruchem ust, a wypowiadaną treścią. Reprezentacje przykładów ze zbioru wsparcia zostały zagregowane za pomocą operacji soft-pooling, co pozwoliło na uzyskanie niskowymiarowej reprezentacji wektorowej całego zadania.
                </p>
              
                <h3 style="font-size: 1.5rem; color: var(--black); margin: 3rem 0 1.5rem 0; font-weight: 600;">Architektura hipersieci</h3>
                <div class="image-container small">
                  <img src="assets/hypernetwork3.png" alt="Architektura hipersieci" />
                  <p class="image-caption">Architektura hipersieci generującej dynamiczne aktualizacje wag.</p>
                </div>
                <p>
                  Hipersieć pełni rolę generatora aktualizacji wag. Dzięki temu model jest w stanie szybko i skutecznie dostosować się do charakterystyki nowego mówcy, bez konieczności pełnego ponownego trenowania. Rozdzielenie głównego modelu i generatora wag pozwala na większą elastyczność oraz oszczędność obliczeniową. Taka konstrukcja umożliwia dynamiczną adaptację bez wpływu na resztę sieci przetwarzającej cechy wizualne.
                </p>
              </section>
              <section id="settings">
                <h2>Eksperymenty</h2>
              
                <div class="grid">
                  <div class="card">
                    <h3>Wariant niewidzianych mówców (ang. <em>unseen speakers</em>)</h3>
                    <p>
                      W trybie <strong>unseen speakers</strong> system walidowany jest na podstawie próbek od mówców, którzy nie pojawili się w zbiorze treningowym. W naszym przypadku wydzielono czterech mówców (dwie kobiety i dwóch mężczyzn) jako zbiór testowy, a pozostałe dane wykorzystano do treningu modelu. Taka konfiguracja pozwala sprawdzić zdolność modelu do generalizacji. Jest to kluczowe w przypadku systemów adaptacyjnych, których zadaniem jest rozpoznawanie mowy osób spoza zbioru treningowego.
                    </p>
                  </div>
              
                  <div class="card">
                    <h3>Wariant nakładających się mówców (ang. <em>overlapped speakers</em>)</h3>
                    <p>
                      W ustawieniu <strong>overlapped speakers</strong> dane testowe pochodzą od tych samych mówców co dane treningowe, lecz zawierają ich inne wypowiedzi. Taki scenariusz pozwala ocenić, jak dobrze model potrafi rozpoznawać nowe zdania u znanych już mówców. Stanowi on prostsze zadanie niż tryb unseen, ale nadal wymaga generalizacji na poziomie wypowiedzi.
                    </p>
                  </div>
                </div>
              
                <p>
                    Do naszych eksperymentów wykorzystano dane ze zbioru GRID obejmującego ponad 30 tysięcy nagrań wideo z synchronizowanym dźwiękiem i transkrypcją, zawierającego wypowiedzi 33 mówców. Skupiliśmy się na ustawieniu <strong>unseen</strong>, które stanowi większe wyzwanie niż konfiguracja overlapped, ponieważ testowane są wypowiedzi od mówców niewystępujących w zbiorze treningowym. Pozwala to rzetelnie ocenić zdolność modelu do adaptacji do nowych użytkowników.
                </p>
              </section>
              
              <section id="metrics">
                <h2>Metryka</h2>
                <p>
                  Word Error Rate to standardowa metryka w dziedzinie rozpoznawania mowy, mierzy ona <strong>liczbę niepoprawnie rozpoznanych słów</strong> w stosunku do całkowitej liczby słów w referencyjnej transkrypcji.
                </p>
              
                <div class="method-flow">
                  <h3>Wzór na WER</h3>
                  <div style="text-align: center; font-size: 1.25rem; margin: 2rem 0; font-weight: 600; color: var(--black);">
                    \( WER = \frac{S + D + I}{N} \times 100\% \)
                  </div>
                  
                  <p>
                    <strong>Zastąpienia (S) – ang. substitutions</strong> – liczba słów, które zostały zastąpione innymi słowami.
                  </p>
                  
                  <p>
                    <strong>Usunięcia (D) – ang. deletions</strong> – liczba słów z referencji, które nie zostały rozpoznane przez system. Słowa te są obecne w prawdziwej transkrypcji, ale brakuje ich w predykcji.
                  </p>
                  
                  <p>
                    <strong>Wstawienia (I) – ang. insertions</strong> – liczba dodatkowych słów, które model przewidział, ale nie występują w referencyjnej transkrypcji.
                  </p>

                  <p>
                    <strong>Liczba słów (N)</strong> - całkowita liczba słów w referencyjnej transkrypcji.
                  </p>
                </div>
                
                <p>
                  W kontekście lipreadingu WER jest szczególnie istotny, ponieważ każde słowo ma znaczenie dla zrozumienia komunikatu. Jego niska wartość oznacza, że system skutecznie rozpoznaje większość wypowiadanych słów, co przekłada się na praktyczną użyteczność w rzeczywistych zastosowaniach.
                </p>
              
              </section>
              
              <section id="results">
                <h2>Wyniki</h2>
                <div class="gif-container">
                  <img src="assets/SOTA.gif" alt="State-of-the-art results animation" />
                </div>
                
                <div class="highlight">
                  <h3>Wyniki na poziomie state-of-the-art</h3>
                  <p>
                    Architektura FLipNet osiąga obecnie <strong>najlepsze wyniki w zadaniu odczytywania mowy z ruchu warg</strong>, znacząco przewyższając istniejące rozwiązania.
                  </p>
                </div>
              
              
              
                <div class="image-container">
                  <img src="assets/results.webp" alt="Wyniki działania modelu" />
                  <p class="image-caption">Zestawienie wyników eksperymentalnych – nasz model przewyższa pozostałe podejścia.</p>
                </div>
              
                <p>
                  Przeprowadzone eksperymenty pokazują, że zastosowanie architektury opartej na hipersieci pozwala osiągnąć bezprecedensową skuteczność w zadaniu lipreadingu. Osiągnięte wyniki wykazują wyraźną przewagę nad klasycznymi podejściami w kilku kluczowych obszarach:
                </p>
              
                <div class="grid">
                    <div class="card">
                      <h3>Szybka adaptacja</h3>
                      <p>Model potrafi dostosować się do nowego mówcy na podstawie ograniczonej liczby przykładów, co sprawia, że proces personalizacji jest znacznie bardziej efektywny niż w tradycyjnych podejściach.</p>
                    </div>
                  
                    <div class="card">
                      <h3>Większa dokładność</h3>
                      <p>Zaproponowana architektura poprawia jakość rozpoznawania mowy wizualnej w przypadku różnych użytkowników, lepiej radząc sobie z indywidualnymi cechami twarzy oraz sposobem artykulacji.</p>
                    </div>
                  
                    <div class="card">
                      <h3>Ogólność rozwiązania</h3>
                      <p>Model wykazuje dobrą skuteczność również w przypadku danych pochodzących od wcześniej niewidzianych mówców, co wskazuje na jego zdolność do uogólniania zdobytej wiedzy.</p>
                    </div>
                  </div>
                  
              
                <p>
                  Wyniki te wskazują, że FLipNet ustanawia nowy punkt odniesienia dla systemów lipreadingu. Połączenie wysokiej dokładności, szybkiej adaptacji oraz odporności na zmiany między mówcami czyni go atrakcyjnym rozwiązaniem dla rzeczywistych zastosowań, w których wymagana jest efektywna personalizacja.
                </p>
              
                <!-- <div class="image-container">
                  <img src="assets/qualitive.webp" alt="Analiza jakościowa wyników" />
                  <p class="image-caption">Analiza jakościowa predykcji modelu.</p>
                </div> -->
              </section>              

            <section id="team">
                <h2>Zespół</h2>
                <div class="grid">
                    <div class="card">
                        <h3>Autorzy projektu</h3>
                        <p>
                            <strong>Adam Bednarski</strong> <a href="https://www.linkedin.com/in/x-adam-bednarski/">Linkedin</a><br>
                            <strong>Łukasz Lenkiewicz</strong> <a href="https://www.linkedin.com/in/lukasz-lenkiewicz/">Linkedin</a><br>
                            <strong>Jan Masłowski</strong> <a href="https://www.linkedin.com/in/jan-mas%C5%82owski-bb5830314/">Linkedin</a>
                        </p>
                    </div>

                    <div class="card">
                        <h3>Opiekunowie projektu</h3>
                        <p>
                            <strong>mgr inż. Przemysław Dolata</strong><br>
                            <strong>dr hab. inż. Maciej Zięba</strong>
                        </p>
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <p>&copy; 2025 FLipNet Research Team.</p>
        </footer>
    </div>

    <script>
        // Hamburger menu functionality
        const hamburger = document.getElementById('hamburger');
        const navMenu = document.getElementById('nav-menu');

        hamburger.addEventListener('click', function() {
            hamburger.classList.toggle('active');
            navMenu.classList.toggle('active');
        });

        // Close menu when clicking on a link
        document.querySelectorAll('nav a').forEach(link => {
            link.addEventListener('click', function() {
                hamburger.classList.remove('active');
                navMenu.classList.remove('active');
            });
        });

        // Close menu when clicking outside
        document.addEventListener('click', function(event) {
            if (!hamburger.contains(event.target) && !navMenu.contains(event.target)) {
                hamburger.classList.remove('active');
                navMenu.classList.remove('active');
            }
        });

        // Add scroll effect to navbar
        window.addEventListener('scroll', function() {
            const navbar = document.getElementById('navbar');
            if (window.scrollY > 100) {
                navbar.classList.add('scrolled');
            } else {
                navbar.classList.remove('scrolled');
            }
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('nav a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    const navHeight = document.getElementById('navbar').offsetHeight;
                    const targetPosition = target.offsetTop - navHeight - 20;
                    window.scrollTo({
                        top: targetPosition,
                        behavior: 'smooth'
                    });
                }
            });
        });
    </script>
</body>
</html> 