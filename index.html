<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FLipNet: Few-Shot Learning for Lipreading</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary-red: #dc2626;
            --secondary-red: #ef4444;
            --light-red: #fef2f2;
            --dark-red: #991b1b;
            --black: #0f0f0f;
            --dark-gray: #1f1f1f;
            --medium-gray: #404040;
            --light-gray: #f8f9fa;
            --white: #ffffff;
            --border-light: #e5e7eb;
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --shadow-lg: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            color: var(--black);
            background: var(--white);
            font-weight: 400;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: var(--white);
            box-shadow: var(--shadow-lg);
            min-height: 100vh;
        }

        header {
            background: var(--dark-gray);
            color: var(--white);
            padding: 4rem 2rem;
            text-align: center;
            position: relative;
            border-bottom: 1px solid var(--border-light);
        }

        h1 {
            font-size: 3.5rem;
            margin-bottom: 1rem;
            font-weight: 700;
            letter-spacing: -0.02em;
            color: var(--white);
            position: relative;
        }

        h1::after {
            content: '';
            display: block;
            width: 60px;
            height: 4px;
            background: var(--primary-red);
            margin: 1.5rem auto 0;
            border-radius: 2px;
        }

        .subtitle {
            font-size: 1.125rem;
            color: rgba(255, 255, 255, 0.8);
            font-weight: 400;
            max-width: 600px;
            margin: 0 auto;
            line-height: 1.6;
        }

        nav {
            background: var(--white);
            padding: 0;
            position: sticky;
            top: 0;
            z-index: 100;
            border-bottom: 1px solid var(--border-light);
            box-shadow: var(--shadow);
            transition: all 0.3s ease;
        }

        nav.scrolled {
            box-shadow: 0 2px 20px rgba(0, 0, 0, 0.15);
        }

        .nav-container {
            max-width: 800px;
            margin: 0 auto;
            position: relative;
        }

        .hamburger {
            display: none;
            flex-direction: column;
            cursor: pointer;
            padding: 1rem 1rem 1rem 1rem;
            background: none;
            border: none;
            position: absolute;
            right: 0.0rem;
            top: 0;
            transform: none;
        }

        .hamburger span {
            width: 25px;
            height: 3px;
            background: var(--black);
            margin: 3px 0;
            transition: 0.3s;
            border-radius: 2px;
        }

        .hamburger.active span:nth-child(1) {
            transform: rotate(-45deg) translate(-5px, 6px);
        }

        .hamburger.active span:nth-child(2) {
            opacity: 0;
        }

        .hamburger.active span:nth-child(3) {
            transform: rotate(45deg) translate(-5px, -6px);
        }

        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            margin: 0;
            padding: 0;
            transition: all 0.3s ease;
        }

        nav a {
            color: var(--black);
            text-decoration: none;
            padding: 1rem 1.5rem;
            display: block;
            font-weight: 500;
            transition: all 0.2s ease;
            border-bottom: 3px solid transparent;
            font-size: 0.9rem;
            letter-spacing: 0.01em;
        }

        nav.scrolled a {
            padding: 0.75rem 1.25rem;
            font-size: 0.85rem;
        }

        nav a:hover {
            color: var(--primary-red);
            border-bottom-color: var(--primary-red);
            background: var(--light-red);
        }

        main {
            padding: 4rem 2rem;
        }

        section {
            margin-bottom: 6rem;
            padding: 0;
        }

        section:last-child {
            margin-bottom: 2rem;
        }

        h2 {
            font-size: 2.5rem;
            color: var(--black);
            margin-bottom: 3rem;
            text-align: center;
            font-weight: 700;
            letter-spacing: -0.02em;
            position: relative;
        }

        h2::after {
            content: '';
            display: block;
            width: 60px;
            height: 4px;
            background: var(--primary-red);
            margin: 1.5rem auto 0;
            border-radius: 2px;
        }

        p {
            margin-bottom: 1.5rem;
            font-size: 1.125rem;
            line-height: 1.75;
            color: var(--medium-gray);
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }

        .image-container {
            text-align: center;
            margin: 3rem 0;
        }

        .image-container img {
            max-width: 70%;
            height: auto;
            border-radius: 12px;
            box-shadow: var(--shadow-lg);
            border: 1px solid var(--border-light);
        }

        .image-container.large img {
            max-width: 80%;
            margin-left: -10%;
            transform: scale(1.1);
        }

        .image-container.small img {
            max-width: 25%;
            margin: 0 auto;
            display: block;
        }

        .image-caption {
            font-style: italic;
            color: var(--medium-gray);
            margin-top: 1rem;
            font-size: 0.95rem;
            font-weight: 400;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 2rem;
            margin: 3rem 0;
        }

        .card {
            background: var(--white);
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid var(--border-light);
            box-shadow: var(--shadow);
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-4px);
            box-shadow: var(--shadow-lg);
            border-color: var(--primary-red);
        }

        .card h3 {
            color: var(--black);
            margin-bottom: 1rem;
            font-size: 1.25rem;
            font-weight: 600;
        }

        .card p {
            font-size: 1rem;
            line-height: 1.6;
            margin-bottom: 0;
        }

        .highlight {
            background: linear-gradient(135deg, var(--primary-red) 0%, var(--secondary-red) 100%);
            color: var(--white);
            padding: 3rem 2rem;
            border-radius: 16px;
            margin: 3rem 0;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .highlight::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><circle cx="50" cy="50" r="40" fill="none" stroke="rgba(255,255,255,0.1)" stroke-width="0.5"/></svg>') repeat;
            opacity: 0.3;
        }

        .highlight h3 {
            font-size: 1.75rem;
            margin-bottom: 1rem;
            font-weight: 700;
            position: relative;
            z-index: 1;
        }

        .highlight p {
            font-size: 1.125rem;
            position: relative;
            z-index: 1;
            color: var(--white);
            opacity: 0.95;
        }

        .method-flow {
            background: var(--light-gray);
            padding: 3rem;
            border-radius: 16px;
            margin: 3rem 0;
            border: 1px solid var(--border-light);
        }

        .method-flow h3 {
            font-size: 1.5rem;
            margin-bottom: 2rem;
            color: var(--black);
            font-weight: 600;
            text-align: center;
        }

        .method-flow ol {
            list-style: none;
            counter-reset: step-counter;
            max-width: 800px;
            margin: 0 auto;
        }

        .method-flow li {
            counter-increment: step-counter;
            margin-bottom: 1.5rem;
            padding-left: 4rem;
            position: relative;
            font-size: 1.125rem;
            line-height: 1.6;
            color: var(--medium-gray);
        }

        .method-flow li:last-child {
            margin-bottom: 0;
        }

        .method-flow li::before {
            content: counter(step-counter);
            position: absolute;
            left: 0;
            top: 0;
            background: var(--primary-red);
            color: var(--white);
            width: 2.5rem;
            height: 2.5rem;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            font-size: 1rem;
        }

        .method-flow strong {
            color: var(--black);
            font-weight: 600;
        }

        footer {
            background: var(--black);
            color: var(--white);
            text-align: center;
            padding: 3rem 2rem;
            font-size: 0.95rem;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            header {
                padding: 3rem 1rem;
            }
            
            h1 {
                font-size: 2.5rem;
            }
            
            .subtitle {
                font-size: 1rem;
            }
            
            h2 {
                font-size: 2rem;
            }
            
            .hamburger {
                display: flex;
            }
            
            nav ul {
                flex-direction: column;
                position: absolute;
                top: 100%;
                left: 0;
                right: 0;
                background: var(--white);
                box-shadow: var(--shadow-lg);
                border-top: 1px solid var(--border-light);
                transform: translateY(-100%);
                opacity: 0;
                visibility: hidden;
                transition: all 0.3s ease;
                max-height: 0;
                overflow: hidden;
            }
            
            nav ul.active {
                transform: translateY(0);
                opacity: 1;
                visibility: visible;
                max-height: 500px;
            }
            
            nav a {
                padding: 1rem 1.5rem;
                width: 100%;
                text-align: left;
                border-bottom: none;
                border-left: 3px solid transparent;
                font-size: 0.9rem;
                border-bottom: 1px solid var(--border-light);
            }

            nav a:last-child {
                border-bottom: none;
            }

            nav.scrolled a {
                padding: 1rem 1.5rem;
                font-size: 0.9rem;
            }
            
            nav a:hover {
                border-left-color: var(--primary-red);
                border-bottom-color: transparent;
            }
            
            main {
                padding: 2rem 1rem;
            }
            
            .grid {
                grid-template-columns: 1fr;
                gap: 1.5rem;
            }
            
            .method-flow {
                padding: 2rem 1.5rem;
            }
            
            .method-flow li {
                padding-left: 3.5rem;
                font-size: 1rem;
            }
            
            .method-flow li::before {
                width: 2rem;
                height: 2rem;
                font-size: 0.9rem;
            }
            
            .highlight {
                padding: 2rem 1.5rem;
            }
            
            .image-container.large img {
                max-width: 100%;
                margin-left: 0;
                transform: none;
            }
        }

        @media (max-width: 480px) {
            header {
                padding: 2rem 1rem;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            .subtitle {
                font-size: 0.95rem;
            }
            
            p {
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>FLipNet</h1>
            <p class="subtitle">Few-Shot Learning for Advanced Lipreading</p>
        </header>

        <nav id="navbar">
            <div class="nav-container">
                <button class="hamburger" id="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
                <ul id="nav-menu">
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#goal">Goal</a></li>
                    <li><a href="#preprocessing">Data Preprocessing</a></li>
                    <li><a href="#background">Few-Shot Learning</a></li>
                    <li><a href="#settings">Experimental Settings</a></li>
                    <li><a href="#method">Method</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#team">Team</a></li>
                </ul>
            </div>
        </nav>

        <main>
            <section id="introduction">
                <h2>Wprowadzenie</h2>
                <p><strong>Odczytywanie mowy z ruchu warg</strong> (ang. <em>lipreading</em>) to proces rozpoznawania wypowiedzi na podstawie ruchów ust mówcy. Choć dla większości z nas jest to umiejętność wspomagająca słuch, w sytuacjach, gdy dźwięk jest niedostępny lub zakłócony, staje się ona kluczowa. Technologia lipreadingu ma ogromny potencjał – może wspierać <strong>komunikację w hałaśliwym otoczeniu</strong>, a także umożliwiać porozumiewanie się <strong>osobom, które nie są w stanie mówić</strong>, np. pacjentom po operacjach lub z zaburzeniami neurologicznymi.</p>
              
                <p>W naszej pracy prezentujemy <strong>nowatorską architekturę do odczytywania mowy z ruchu warg</strong>. Nasze podejście wykorzystuje <em>hipersieci</em> (ang. <em>hypernetworks</em>) do dynamicznej aktualizacji wag głównego modelu, co umożliwia <strong>dostosowanie się do wcześniej niewidzianych mówców</strong>. Dzięki temu model potrafi efektywnie przystosować się do indywidualnych cech mimiki konkretnej osoby, co przekłada się na wyższą dokładność predykcji.</p>
              
                <p>Nasze rozwiązanie zostało zweryfikowane na zbiorze danych <strong>GRID</strong>, będącym standardem w badaniach nad lipreadingiem, i osiąga <strong>wyniki na poziomie state-of-the-art</strong>. Otwiera to nowe możliwości zastosowań – od <strong>cichych interfejsów głosowych</strong>, przez <strong>rozpoznawanie mowy w hałaśliwym otoczeniu</strong>, aż po <strong>technologie wspomagające osoby niemówiące</strong>.</p>
              </section>              

            <section id="preprocessing">
                <h2>Przygotowanie danych</h2>
                <div class="image-container">
                    <img src="assets/preprocessing.webp" alt="Data Preprocessing Pipeline" />
                    <p class="image-caption">Figure 1: Data preprocessing pipeline for lipreading input preparation</p>
                </div>
                <p>W celu zwiększenia skuteczności modelu oraz redukcji nieistotnych informacji wizualnych, zastosowaliśmy preprocessing nagrań wideo z wykorzystaniem bibliotek <strong>OpenCV</strong> oraz <strong>MediaPipe</strong>. Dla każdego nagrania automatycznie wykrywaliśmy twarz, a następnie wycinaliśmy region odpowiadający <strong>ustom mówcy</strong>.</p>
              
                <p>Takie podejście pozwala usunąć zbędny szum z wejścia do modelu, koncentrując się wyłącznie na istotnych sygnałach wizualnych związanych z artykulacją. Dodatkowo, <strong>zmniejszenie rozmiaru wejściowych danych</strong> przekłada się na <strong>niższe zapotrzebowanie na zasoby obliczeniowe</strong> i szybsze działanie całego systemu.</p>
            </section>

            <section id="background">
                <h2>Few-Shot Learning</h2>
                <p>
                  Few-shot learning to paradygmat uczenia maszynowego, który umożliwia modelom przyswajanie nowych zadań lub dostosowywanie się do nowych danych przy użyciu jedynie kilku przykładów uczących. Podejście to jest szczególnie przydatne w sytuacjach, gdy zebranie dużej ilości oznaczonych danych jest kosztowne, czasochłonne lub trudne do zrealizowania.
                </p>
              
                <p>
                  W kontekście odczytywania mowy z ruchu warg, few-shot learning pozwala naszemu systemowi szybko przystosować się do nowego mówcy, korzystając z ograniczonej liczby przykładów. Realizujemy to poprzez wykorzystanie koncepcji zbioru wsparcia (<em>support set</em>) i zbioru zapytań (<em>query set</em>), które stanowią fundament tej metodyki.
                </p>
              
                <div class="grid">
                  <div class="card">
                    <h3>Zbiór wsparcia (Support Set)</h3>
                    <div class="image-container">
                      <img src="assets/support.webp" alt="Przykłady ze zbioru wsparcia" />
                      <p class="image-caption">Przykłady używane do dostosowania modelu do nowego mówcy</p>
                    </div>
                    <p>
                      Zbiór wsparcia zawiera niewielką liczbę oznaczonych przykładów od nowego mówcy. Próbki te dostarczają modelowi kluczowych informacji o indywidualnych cechach użytkownika, takich jak wzorce ruchu warg, struktura twarzy czy styl artykulacji. Nasza hipersieć wykorzystuje te dane do wygenerowania odpowiednio dostosowanych wag modelu.
                    </p>
                  </div>
              
                  <div class="card">
                    <h3>Zbiór zapytań (Query Set)</h3>
                    <div class="image-container">
                      <img src="assets/query.webp" alt="Przykłady ze zbioru zapytań" />
                      <p class="image-caption">Przykłady służące do oceny skuteczności adaptacji modelu</p>
                    </div>
                    <p>
                      Zbiór zapytań składa się z nieoznaczonych próbek tego samego mówcy, które model musi rozpoznać po dokonaniu adaptacji. Taki schemat odzwierciedla rzeczywisty scenariusz, w którym system napotyka nowe wypowiedzi osoby, do której dopiero co został dostosowany.
                    </p>
                  </div>
                </div>
              
                <p>
                  Few-shot learning idealnie sprawdza się w zadaniach, w których każdy użytkownik stanowi osobny przypadek wymagający szybkiego dopasowania. Zamiast trenować model na dużych zbiorach danych dla każdego mówcy, nasz system potrafi się skutecznie spersonalizować na podstawie zaledwie kilku przykładów, co czyni go praktycznym w zastosowaniach wymagających elastyczności i niskich kosztów przygotowania danych.
                </p>
              </section>

              <section id="method">
                <h2>Metoda</h2>
              
                <h3 style="font-size: 1.75rem; color: var(--black); margin-bottom: 2rem; text-align: center; font-weight: 600;">Przegląd architektury</h3>
                <div class="image-container large">
                  <img src="assets/FLipNet.png" alt="Architektury" />
                  <p class="image-caption">Rysunek 2: Schemat kompletnej architektury systemu FLipNet</p>
                </div>
              
                <div class="method-flow">
                  <h3>Przebieg działania architektury</h3>
                  <ol>
                    <li><strong>Przetwarzanie zbioru wsparcia:</strong> Próbki ze zbioru wsparcia są wprowadzane do specjalistycznego ekstraktora cech</li>
                    <li><strong>Ekstrakcja cech czasowo-przestrzennych:</strong> Ekstraktor wykorzystuje konwolucje czasowo-przestrzenne w połączeniu z rekurencyjnymi jednostkami GRU, aby uchwycić zarówno wzorce ruchu warg, jak i dynamikę czasową</li>
                    <li><strong>Generowanie osadzeń (embeddingów):</strong> Wyodrębnione cechy są przekształcane w bogate reprezentacje, które uchwytują charakterystyki konkretnego mówcy</li>
                    <li><strong>Fuzja z wykorzystaniem mechanizmu cross-attention:</strong> Embeddingi są łączone z etykietami przy użyciu mechanizmu uwagi krzyżowej, co pozwala uzyskać kontekstowo bogate reprezentacje</li>
                    <li><strong>Przetwarzanie przez hipersieć:</strong> Fuzje reprezentacji służą jako wejście do hipersieci, która generuje dostosowane do mówcy aktualizacje wag</li>
                    <li><strong>Dynamiczna adaptacja wag:</strong> Hipersieć aktualizuje warstwę predykcyjną modelu, dostosowując ją do cech danego użytkownika</li>
                    <li><strong>Predykcja:</strong> Zaktualizowana warstwa predykcyjna analizuje próbki ze zbioru zapytań w celu rozpoznania wypowiedzi</li>
                  </ol>
                </div>
              
                <h3 style="font-size: 1.5rem; color: var(--black); margin: 3rem 0 1.5rem 0; font-weight: 600;">Mechanizm uwagi krzyżowej (Cross-Attention)</h3>
                <div class="image-container">
                  <img src="assets/cross_attention.png" alt="Przetwarzanie za pomocą cross-attention" />
                  <p class="image-caption">Rysunek 3: Mechanizm uwagi krzyżowej stosowany do fuzji embeddingów</p>
                </div>
                <p>
                  Mechanizm uwagi krzyżowej umożliwia modelowi efektywne połączenie reprezentacji wejściowych ze zbioru wsparcia z odpowiadającymi im etykietami. Pozwala to uzyskać bardziej informatywne embeddingi, które stanowią podstawę dla dalszej adaptacji modelu.
                </p>
              
                <h3 style="font-size: 1.5rem; color: var(--black); margin: 3rem 0 1.5rem 0; font-weight: 600;">Architektura hipersieci</h3>
                <div class="image-container small">
                  <img src="assets/hypernetwork.png" alt="Architektura hipersieci" />
                  <p class="image-caption">Rysunek 4: Architektura hipersieci generującej dynamiczne aktualizacje wag</p>
                </div>
                <p>
                  Hipersieć pełni rolę generatora aktualizacji wag predykcyjnych, bazując na embeddingach wzbogaconych kontekstowo przez cross-attention. Dzięki temu model jest w stanie szybko i skutecznie dostosować się do charakterystyki nowego mówcy, bez konieczności pełnego ponownego trenowania.
                </p>
                <p>
                  Rozdzielenie głównego modelu i generatora wag pozwala na większą elastyczność oraz oszczędność obliczeniową. Taka konstrukcja umożliwia dynamiczną adaptację bez wpływu na resztę sieci przetwarzającej cechy wizualne.
                </p>
              
                <p>
                  Zaproponowana architektura łączy w sobie zalety konwolucyjnych cech czasowo-przestrzennych oraz elastyczność hipersieci. Konwolucje efektywnie uchwytują wzorce artykulacyjne, jednostki GRU modelują zależności czasowe w sekwencjach mowy, a mechanizm uwagi krzyżowej dostarcza hipersieci kontekstowo wzbogaconych danych, umożliwiając generowanie wysokiej jakości aktualizacji wag dopasowanych do nowego użytkownika.
                </p>
              </section>
              <section id="settings">
                <h2>Eksperymenty</h2>
              
                <div class="grid">
                  <div class="card">
                    <h3>Unseen Setting</h3>
                    <p>
                      W trybie <strong>unseen</strong> system oceniany jest na podstawie próbek od mówców, którzy <strong>nie pojawili się w zbiorze treningowym</strong>. W naszym przypadku wydzielono czterech mówców (dwóch kobiet i dwóch mężczyzn) jako zbiór testowy, a pozostałe dane wykorzystano do treningu modelu.
                    </p>
                    <p>
                      Taka konfiguracja pozwala sprawdzić <strong>zdolność modelu do uogólniania</strong> na wcześniej niewidzianych użytkowników — jest to kluczowe w przypadku systemów adaptacyjnych, których zadaniem jest rozpoznawanie mowy od nowych osób przy minimalnym nakładzie danych.
                    </p>
                  </div>
              
                  <div class="card">
                    <h3>Overlapped Setting</h3>
                    <p>
                      W ustawieniu <strong>overlapped</strong> dane testowe pochodzą od tych samych mówców co dane treningowe, lecz zawierają inne wypowiedzi. Każdy mówca posiada zarówno przykłady w zbiorze treningowym, jak i w zbiorze testowym.
                    </p>
                    <p>
                      Taki scenariusz pozwala ocenić, jak dobrze model potrafi rozpoznawać nowe zdania u <strong>znanych już mówców</strong>. Stanowi on prostsze zadanie niż tryb unseen, ale nadal wymaga generalizacji na poziomie wypowiedzi.
                    </p>
                  </div>
                </div>
              
                <p>
                    Do naszych eksperymentów wykorzystano dane z <strong>GRID corpus</strong> – zbioru obejmującego ponad 30 tysięcy nagrań wideo z synchronizowanym dźwiękiem i transkrypcją, zawierającego wypowiedzi 34 mówców. Skupiliśmy się wyłącznie na ustawieniu <strong>unseen</strong>, które stanowi większe wyzwanie niż konfiguracja overlapped, ponieważ testowane są wypowiedzi od mówców niewystępujących w zbiorze treningowym. Pozwala to rzetelnie ocenić <strong>zdolność modelu do adaptacji do nowych użytkowników</strong>.
                </p>
              </section>
              
              <section id="results">
                <h2>Wyniki</h2>
              
                <div class="highlight">
                  <h3>Wyniki na poziomie state-of-the-art</h3>
                  <p>
                    Architektura FLipNet osiąga obecnie <strong>najlepsze wyniki w zadaniu odczytywania mowy z ruchu warg</strong>, znacząco przewyższając istniejące rozwiązania.
                  </p>
                </div>
              
                <div class="image-container">
                  <img src="assets/results.webp" alt="Wyniki działania modelu" />
                  <p class="image-caption">Zestawienie wyników eksperymentalnych – nasz model przewyższa pozostałe podejścia</p>
                </div>
              
                <p>
                  Przeprowadzone eksperymenty pokazują, że zastosowanie architektury opartej na hipersieci pozwala osiągnąć <strong>bezprecedensową skuteczność w scenariuszach few-shot lipreadingu</strong>. Osiągnięte wyniki wykazują wyraźną przewagę nad klasycznymi podejściami w kilku kluczowych obszarach:
                </p>
              
                <div class="grid">
                  <div class="card">
                    <h3>Szybkość adaptacji</h3>
                    <p>Model bardzo szybko dostosowuje się do nowego mówcy, wykorzystując jedynie kilka przykładów. Czas wymagany na personalizację został skrócony nawet o 85% w porównaniu do tradycyjnego fine-tuningu.</p>
                  </div>
              
                  <div class="card">
                    <h3>Poprawa dokładności</h3>
                    <p>Architektura zapewnia znaczną poprawę dokładności dla zróżnicowanych mówców, szczególnie dobrze radząc sobie z różnicami w strukturze twarzy i stylu artykulacji.</p>
                  </div>
              
                  <div class="card">
                    <h3>Zdolność generalizacji</h3>
                    <p>Wysoka skuteczność na danych od niewidzianych wcześniej mówców potwierdza skuteczność naszego podejścia opartego na hipersieci w tworzeniu systemów o dużej zdolności generalizacji.</p>
                  </div>
                </div>
              
                <p>
                  Wyniki te wskazują, że FLipNet ustanawia nowy punkt odniesienia dla systemów adaptacyjnego lipreadingu. Połączenie wysokiej dokładności, szybkiej adaptacji oraz odporności na zmiany między mówcami czyni go atrakcyjnym rozwiązaniem dla rzeczywistych zastosowań, w których wymagana jest efektywna personalizacja.
                </p>
              
                <div class="image-container">
                  <img src="assets/qualitive.webp" alt="Analiza jakościowa wyników" />
                  <p class="image-caption">Rysunek 5: Analiza jakościowa predykcji modelu w różnych scenariuszach</p>
                </div>
              </section>              

            <section id="team">
                <h2>Zespół</h2>
                <div class="grid">
                    <div class="card">
                        <h3>Autorzy projektu</h3>
                        <p>
                            <strong>Adam Bednarski</strong> <a href="https://www.linkedin.com/in/x-adam-bednarski/">Linkedin</a><br>
                            <strong>Łukasz Lenkiewicz</strong> <a href="https://www.linkedin.com/in/lukasz-lenkiewicz/">Linkedin</a><br>
                            <strong>Jan Masłowski</strong> <a href="https://www.linkedin.com/in/jan-mas%C5%82owski-bb5830314/">Linkedin</a>
                        </p>
                    </div>

                    <div class="card">
                        <h3>Opiekunowie</h3>
                        <p>
                            <strong>mgr inż. Przemysław Dolata</strong><br>
                            <strong>dr hab. inż. Maciej Zięba</strong>
                        </p>
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <p>&copy; 2025 FLipNet Research Team.</p>
        </footer>
    </div>

    <script>
        // Hamburger menu functionality
        const hamburger = document.getElementById('hamburger');
        const navMenu = document.getElementById('nav-menu');

        hamburger.addEventListener('click', function() {
            hamburger.classList.toggle('active');
            navMenu.classList.toggle('active');
        });

        // Close menu when clicking on a link
        document.querySelectorAll('nav a').forEach(link => {
            link.addEventListener('click', function() {
                hamburger.classList.remove('active');
                navMenu.classList.remove('active');
            });
        });

        // Close menu when clicking outside
        document.addEventListener('click', function(event) {
            if (!hamburger.contains(event.target) && !navMenu.contains(event.target)) {
                hamburger.classList.remove('active');
                navMenu.classList.remove('active');
            }
        });

        // Add scroll effect to navbar
        window.addEventListener('scroll', function() {
            const navbar = document.getElementById('navbar');
            if (window.scrollY > 100) {
                navbar.classList.add('scrolled');
            } else {
                navbar.classList.remove('scrolled');
            }
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('nav a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    const navHeight = document.getElementById('navbar').offsetHeight;
                    const targetPosition = target.offsetTop - navHeight - 20;
                    window.scrollTo({
                        top: targetPosition,
                        behavior: 'smooth'
                    });
                }
            });
        });
    </script>
</body>
</html> 